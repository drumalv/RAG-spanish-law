{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llama index evaluation\n",
    "https://www.llamaindex.ai/blog/evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "import tqdm\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from llama_index.core import (\n",
    "    SimpleDirectoryReader,\n",
    "    VectorStoreIndex,\n",
    "    ServiceContext,\n",
    "    Document\n",
    ")\n",
    "from llama_index.core.evaluation import (\n",
    "    DatasetGenerator,\n",
    "    FaithfulnessEvaluator,\n",
    "    RelevancyEvaluator,\n",
    "    ContextRelevancyEvaluator\n",
    ")\n",
    "from llama_index.llms.azure_openai import AzureOpenAI\n",
    "from llama_index.embeddings.azure_openai import AzureOpenAIEmbedding\n",
    "\n",
    "import time\n",
    "from llama_index.core import Settings\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../data/Constitución Española'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load documents splitted by articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = path + '/chunks/documents_spanisharticlesplitter.pkl'\n",
    "# Load the texts from the pickle file\n",
    "with open(file_path, \"rb\") as file:\n",
    "    langdocs = pickle.load(file)\n",
    "\n",
    "# Documents to llama-index docs\n",
    "llamadocs = [Document(text=doc.page_content, metadata=doc.metadata) for doc in langdocs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = AzureOpenAI(\n",
    "    openai_api_version=os.environ[\"OPENAI_API_VERSION\"],\n",
    "    azure_deployment=os.environ[\"AZURE_GPT3TURBO_DEPLOYMENT\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.prompts.base import PromptTemplate\n",
    "\n",
    "QUESTION_GENERATION_PROMPT = PromptTemplate(\"\"\"\\\n",
    "Context information is below.\n",
    "---------------------\n",
    "{context_str}\n",
    "---------------------\n",
    "Given the context information and not prior knowledge.\n",
    "generate only questions based on the below query.\n",
    "Generate the questions in Spanish.\n",
    "{query_str}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alvaro/tfm/.venv/lib/python3.11/site-packages/llama_index/core/evaluation/dataset_generation.py:213: DeprecationWarning: Call to deprecated class DatasetGenerator. (Deprecated in favor of `RagDatasetGenerator` which should be used instead.)\n",
      "  return cls(\n",
      "/home/alvaro/tfm/.venv/lib/python3.11/site-packages/llama_index/core/evaluation/dataset_generation.py:310: DeprecationWarning: Call to deprecated class QueryResponseDataset. (Deprecated in favor of `LabelledRagDataset` which should be used instead.)\n",
      "  return QueryResponseDataset(queries=queries, responses=responses_dict)\n"
     ]
    }
   ],
   "source": [
    "data_generator = DatasetGenerator.from_documents(documents=llamadocs, llm=llm, text_question_template=QUESTION_GENERATION_PROMPT)\n",
    "eval_questions = data_generator.generate_questions_from_nodes(num=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a DataFrame from the eval_questions list\n",
    "df = pd.DataFrame(eval_questions, columns=['Questions'])\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv(path + '/eval_questions.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eval Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1938/127690951.py:15: DeprecationWarning: Call to deprecated class method from_defaults. (ServiceContext is deprecated, please use `llama_index.settings.Settings` instead.) -- Deprecated since version 0.10.0.\n",
      "  service_context_gpt4 = ServiceContext.from_defaults(llm=gpt4, embed_model=embed_model)\n"
     ]
    }
   ],
   "source": [
    "# We will use GPT-4 for evaluating the responses\n",
    "gpt4 = AzureOpenAI(\n",
    "    openai_api_version=os.environ[\"OPENAI_API_VERSION\"],\n",
    "    azure_deployment=os.environ[\"AZURE_GPT4TURBO_DEPLOYMENT\"],\n",
    "    api_key=os.environ[\"AZURE_OPENAI_API_KEY\"]\n",
    ")\n",
    "\n",
    "embed_model = AzureOpenAIEmbedding(\n",
    "    openai_api_version=os.environ[\"OPENAI_API_VERSION\"],\n",
    "    azure_deployment=os.environ[\"AZURE_ADA2_DEPLOYMENT\"],\n",
    "    api_key=os.environ[\"AZURE_OPENAI_API_KEY\"]\n",
    ")\n",
    "\n",
    "# Define service context for GPT-4 for evaluation\n",
    "service_context_gpt4 = ServiceContext.from_defaults(llm=gpt4, embed_model=embed_model)\n",
    "\n",
    "# Define Faithfulness and Relevancy Evaluators which are based on GPT-4\n",
    "faithfulness_gpt4 = FaithfulnessEvaluator(service_context=service_context_gpt4)\n",
    "relevancy_gpt4 = RelevancyEvaluator(service_context=service_context_gpt4)\n",
    "contextrelevancy_gpt4 = ContextRelevancyEvaluator(service_context=service_context_gpt4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def evaluate_response_time_and_accuracy(eval_documents, eval_questions):\n",
    "    \"\"\"\n",
    "    Evaluate the average response time, faithfulness, and relevancy of responses generated by GPT-3.5-turbo for given documents.\n",
    "    \n",
    "    Parameters:\n",
    "    eval_documents (list): The list of pre-split documents to be evaluated.\n",
    "    eval_questions (list): The list of questions to evaluate responses for.\n",
    "    \n",
    "    Returns:\n",
    "    tuple: A tuple containing the average response time, faithfulness, and relevancy metrics.\n",
    "    \"\"\"\n",
    "\n",
    "    total_response_time = 0\n",
    "    total_faithfulness = 0\n",
    "    total_relevancy = 0\n",
    "    total_contextrelevancy = 0.0\n",
    "\n",
    "    # Create vector index\n",
    "    llm = AzureOpenAI(\n",
    "        openai_api_version=os.environ[\"OPENAI_API_VERSION\"],\n",
    "        azure_deployment=os.environ[\"AZURE_GPT3TURBO_DEPLOYMENT\"],\n",
    "        api_key=os.environ[\"AZURE_OPENAI_API_KEY\"]\n",
    "    )\n",
    "    service_context = ServiceContext.from_defaults(llm=llm, embed_model=embed_model)\n",
    "    vector_index = VectorStoreIndex.from_documents(\n",
    "        eval_documents, service_context=service_context\n",
    "    )\n",
    "    # Build query engine\n",
    "    query_engine = vector_index.as_query_engine()\n",
    "    num_questions = len(eval_questions)\n",
    "\n",
    "    # Iterate over each question in eval_questions to compute metrics.\n",
    "    # While BatchEvalRunner can be used for faster evaluations (see: https://docs.llamaindex.ai/en/latest/examples/evaluation/batch_eval.html),\n",
    "    # we're using a loop here to specifically measure response time for different chunk sizes.\n",
    "\n",
    "    df = []\n",
    "\n",
    "    for question in tqdm.tqdm(eval_questions):\n",
    "\n",
    "        start_time = time.time()\n",
    "        response_vector = query_engine.query(question)\n",
    "        elapsed_time = time.time() - start_time\n",
    "        \n",
    "        faithfulness_result = faithfulness_gpt4.evaluate_response(\n",
    "            response=response_vector\n",
    "        ).passing\n",
    "        \n",
    "        relevancy_result = relevancy_gpt4.evaluate_response(\n",
    "            query=question, response=response_vector\n",
    "        ).passing\n",
    "\n",
    "        contextrelevancy_result = await contextrelevancy_gpt4.aevaluate_response(question, response_vector)\n",
    "\n",
    "        total_response_time += elapsed_time\n",
    "        total_faithfulness += faithfulness_result\n",
    "        total_relevancy += relevancy_result\n",
    "        total_contextrelevancy += contextrelevancy_result.score\n",
    "\n",
    "\n",
    "        data = {'question': question, \n",
    "                'response': response_vector.response,\n",
    "                'context': [{'text': c.text, 'metadata': c.metadata} for c in response_vector.source_nodes],\n",
    "                'faithfulness': faithfulness_result, \n",
    "                'relevancy': relevancy_result,\n",
    "                'context_relevancy_score': contextrelevancy_result.score,\n",
    "                'context_relevancy_feedback': contextrelevancy_result.feedback,\n",
    "                'response_time': elapsed_time}\n",
    "\n",
    "        df.append(data.copy())\n",
    "\n",
    "    average_response_time = total_response_time / num_questions\n",
    "    average_faithfulness = total_faithfulness / num_questions\n",
    "    average_relevancy = total_relevancy / num_questions\n",
    "    average_contextrelevancy = total_contextrelevancy / num_questions\n",
    "\n",
    "    return average_response_time, average_faithfulness, average_relevancy, average_contextrelevancy, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "CHUNKERIZATION FILE:  documents_spanisharticlesplitter.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1938/1377853279.py:24: DeprecationWarning: Call to deprecated class method from_defaults. (ServiceContext is deprecated, please use `llama_index.settings.Settings` instead.) -- Deprecated since version 0.10.0.\n",
      "  service_context = ServiceContext.from_defaults(llm=llm, embed_model=embed_model)\n",
      "100%|██████████| 300/300 [56:04<00:00, 11.21s/it] \n",
      "/tmp/ipykernel_1938/1377853279.py:24: DeprecationWarning: Call to deprecated class method from_defaults. (ServiceContext is deprecated, please use `llama_index.settings.Settings` instead.) -- Deprecated since version 0.10.0.\n",
      "  service_context = ServiceContext.from_defaults(llm=llm, embed_model=embed_model)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "CHUNKERIZATION FILE:  documents_charactersplitter_400.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [44:01<00:00,  8.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "CHUNKERIZATION FILE:  documents_recursivecharactersplitter_400.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [44:34<00:00,  8.92s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "CHUNKERIZATION FILE:  documents_recursivecharactersplitter_200.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [42:37<00:00,  8.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "CHUNKERIZATION FILE:  documents_charactersplitter_300.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [34:42<00:00,  6.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "CHUNKERIZATION FILE:  documents_recursivecharactersplitter_300.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [38:06<00:00,  7.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "CHUNKERIZATION FILE:  documents_charactersplitter_200.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [36:58<00:00,  7.40s/it]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv( path + '/eval_questions.csv')\n",
    "eval_questions = df['Questions'].tolist()\n",
    "\n",
    "results = {}\n",
    "directory = path + '/chunks'\n",
    "for filename in os.listdir(directory):\n",
    "    print(\"-----------------------------------\")\n",
    "    print(\"CHUNKERIZATION FILE: \", filename)\n",
    "    file_path = os.path.join(directory, filename)\n",
    "    with open(file_path, \"rb\") as file:\n",
    "        langdocs = pickle.load(file)\n",
    "    # Documents to llama-index docs\n",
    "    llamadocs = [Document(text=doc.page_content, metadata=doc.metadata) for doc in langdocs]\n",
    "    avg_response_time, avg_faithfulness, avg_relevancy, average_contextrelevancy, df = await evaluate_response_time_and_accuracy(llamadocs, eval_questions)\n",
    "    results[filename] = { \"Average response time\": avg_response_time, \"Average faithfulness\": avg_faithfulness, \"Average relevancy\": avg_relevancy , \"Average context relevancy\": average_contextrelevancy}\n",
    "\n",
    "    file_path = os.path.join(path, 'eval_chunks', filename.replace('.pkl', '_answers.json'))\n",
    "    # Save the metrics dictionary as a JSON file\n",
    "    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "        json.dump(df, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "\n",
    "file_path = os.path.join(path, 'metrics.json')\n",
    "# Save the metrics dictionary as a JSON file\n",
    "with open(file_path, 'w') as file:\n",
    "    json.dump(results, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "path = '../data/Constitución Española'\n",
    "metrics_file_path = os.path.join(path,'metrics.json')\n",
    "\n",
    "# Read the metrics from the JSON file\n",
    "with open(metrics_file_path, 'r') as file:\n",
    "    metrics = json.load(file)\n",
    "\n",
    "# Create a DataFrame from the metrics dictionary\n",
    "df = pd.DataFrame(metrics).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_324da_row0_col0, #T_324da_row0_col2, #T_324da_row0_col3, #T_324da_row1_col1 {\n",
       "  text-decoration: underline;\n",
       "  color: green;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_324da\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_324da_level0_col0\" class=\"col_heading level0 col0\" >Average response time</th>\n",
       "      <th id=\"T_324da_level0_col1\" class=\"col_heading level0 col1\" >Average faithfulness</th>\n",
       "      <th id=\"T_324da_level0_col2\" class=\"col_heading level0 col2\" >Average relevancy</th>\n",
       "      <th id=\"T_324da_level0_col3\" class=\"col_heading level0 col3\" >Average context relevancy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_324da_level0_row0\" class=\"row_heading level0 row0\" >documents_spanisharticlesplitter.pkl</th>\n",
       "      <td id=\"T_324da_row0_col0\" class=\"data row0 col0\" >1.559313</td>\n",
       "      <td id=\"T_324da_row0_col1\" class=\"data row0 col1\" >0.943333</td>\n",
       "      <td id=\"T_324da_row0_col2\" class=\"data row0 col2\" >0.943333</td>\n",
       "      <td id=\"T_324da_row0_col3\" class=\"data row0 col3\" >0.856458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_324da_level0_row1\" class=\"row_heading level0 row1\" >documents_charactersplitter_400.pkl</th>\n",
       "      <td id=\"T_324da_row1_col0\" class=\"data row1 col0\" >1.521924</td>\n",
       "      <td id=\"T_324da_row1_col1\" class=\"data row1 col1\" >0.950000</td>\n",
       "      <td id=\"T_324da_row1_col2\" class=\"data row1 col2\" >0.906667</td>\n",
       "      <td id=\"T_324da_row1_col3\" class=\"data row1 col3\" >0.717917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_324da_level0_row2\" class=\"row_heading level0 row2\" >documents_recursivecharactersplitter_400.pkl</th>\n",
       "      <td id=\"T_324da_row2_col0\" class=\"data row2 col0\" >1.504329</td>\n",
       "      <td id=\"T_324da_row2_col1\" class=\"data row2 col1\" >0.926667</td>\n",
       "      <td id=\"T_324da_row2_col2\" class=\"data row2 col2\" >0.866667</td>\n",
       "      <td id=\"T_324da_row2_col3\" class=\"data row2 col3\" >0.690417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_324da_level0_row3\" class=\"row_heading level0 row3\" >documents_recursivecharactersplitter_200.pkl</th>\n",
       "      <td id=\"T_324da_row3_col0\" class=\"data row3 col0\" >1.432389</td>\n",
       "      <td id=\"T_324da_row3_col1\" class=\"data row3 col1\" >0.946667</td>\n",
       "      <td id=\"T_324da_row3_col2\" class=\"data row3 col2\" >0.850000</td>\n",
       "      <td id=\"T_324da_row3_col3\" class=\"data row3 col3\" >0.647917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_324da_level0_row4\" class=\"row_heading level0 row4\" >documents_charactersplitter_300.pkl</th>\n",
       "      <td id=\"T_324da_row4_col0\" class=\"data row4 col0\" >1.298296</td>\n",
       "      <td id=\"T_324da_row4_col1\" class=\"data row4 col1\" >0.910000</td>\n",
       "      <td id=\"T_324da_row4_col2\" class=\"data row4 col2\" >0.826667</td>\n",
       "      <td id=\"T_324da_row4_col3\" class=\"data row4 col3\" >0.666250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_324da_level0_row5\" class=\"row_heading level0 row5\" >documents_recursivecharactersplitter_300.pkl</th>\n",
       "      <td id=\"T_324da_row5_col0\" class=\"data row5 col0\" >1.387602</td>\n",
       "      <td id=\"T_324da_row5_col1\" class=\"data row5 col1\" >0.940000</td>\n",
       "      <td id=\"T_324da_row5_col2\" class=\"data row5 col2\" >0.896667</td>\n",
       "      <td id=\"T_324da_row5_col3\" class=\"data row5 col3\" >0.689583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_324da_level0_row6\" class=\"row_heading level0 row6\" >documents_charactersplitter_200.pkl</th>\n",
       "      <td id=\"T_324da_row6_col0\" class=\"data row6 col0\" >1.360128</td>\n",
       "      <td id=\"T_324da_row6_col1\" class=\"data row6 col1\" >0.913333</td>\n",
       "      <td id=\"T_324da_row6_col2\" class=\"data row6 col2\" >0.816667</td>\n",
       "      <td id=\"T_324da_row6_col3\" class=\"data row6 col3\" >0.645833</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7fd2e8ba4b10>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def highlight_best_metrics(s):\n",
    "    styles = []\n",
    "    for val in s:\n",
    "        if val == s.max():\n",
    "            styles.append('text-decoration: underline; color: green')\n",
    "        else:\n",
    "            styles.append('')\n",
    "    return styles\n",
    "\n",
    "# Apply the style function to the DataFrame and display it\n",
    "df.style.apply(highlight_best_metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
