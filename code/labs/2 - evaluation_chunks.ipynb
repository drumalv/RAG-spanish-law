{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llama index evaluation\n",
    "https://www.llamaindex.ai/blog/evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "import tqdm\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from llama_index.core import (\n",
    "    SimpleDirectoryReader,\n",
    "    VectorStoreIndex,\n",
    "    ServiceContext,\n",
    "    Document\n",
    ")\n",
    "from llama_index.core.evaluation import (\n",
    "    DatasetGenerator,\n",
    "    FaithfulnessEvaluator,\n",
    "    RelevancyEvaluator,\n",
    "    ContextRelevancyEvaluator\n",
    ")\n",
    "from llama_index.llms.azure_openai import AzureOpenAI\n",
    "from llama_index.embeddings.azure_openai import AzureOpenAIEmbedding\n",
    "\n",
    "import time\n",
    "from llama_index.core import Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../data/Constitución Española'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load documents splitted by articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "file_path = path + '/chunks/documents_spanisharticlesplitter.pkl'\n",
    "# Load the texts from the pickle file\n",
    "with open(file_path, \"rb\") as file:\n",
    "    langdocs = pickle.load(file)\n",
    "\n",
    "# Documents to llama-index docs\n",
    "llamadocs = [Document(text=doc.page_content, metadata=doc.metadata) for doc in langdocs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = AzureOpenAI(\n",
    "    openai_api_version=os.environ[\"OPENAI_API_VERSION\"],\n",
    "    azure_deployment=os.environ[\"AZURE_GPT3TURBO_DEPLOYMENT\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.prompts.base import PromptTemplate\n",
    "\n",
    "QUESTION_GENERATION_PROMPT = PromptTemplate(\"\"\"\\\n",
    "Context information is below.\n",
    "---------------------\n",
    "{context_str}\n",
    "---------------------\n",
    "Given the context information and not prior knowledge.\n",
    "generate only questions based on the below query.\n",
    "Generate the questions in Spanish.\n",
    "{query_str}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alvaro/tfm/.venv/lib/python3.11/site-packages/llama_index/core/evaluation/dataset_generation.py:213: DeprecationWarning: Call to deprecated class DatasetGenerator. (Deprecated in favor of `RagDatasetGenerator` which should be used instead.)\n",
      "  return cls(\n",
      "/home/alvaro/tfm/.venv/lib/python3.11/site-packages/llama_index/core/evaluation/dataset_generation.py:310: DeprecationWarning: Call to deprecated class QueryResponseDataset. (Deprecated in favor of `LabelledRagDataset` which should be used instead.)\n",
      "  return QueryResponseDataset(queries=queries, responses=responses_dict)\n"
     ]
    }
   ],
   "source": [
    "data_generator = DatasetGenerator.from_documents(documents=llamadocs, llm=llm, text_question_template=QUESTION_GENERATION_PROMPT)\n",
    "eval_questions = data_generator.generate_questions_from_nodes(num=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a DataFrame from the eval_questions list\n",
    "df = pd.DataFrame(eval_questions, columns=['Questions'])\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv(path + '/eval_questions.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eval Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2359/127690951.py:15: DeprecationWarning: Call to deprecated class method from_defaults. (ServiceContext is deprecated, please use `llama_index.settings.Settings` instead.) -- Deprecated since version 0.10.0.\n",
      "  service_context_gpt4 = ServiceContext.from_defaults(llm=gpt4, embed_model=embed_model)\n"
     ]
    }
   ],
   "source": [
    "# We will use GPT-4 for evaluating the responses\n",
    "gpt4 = AzureOpenAI(\n",
    "    openai_api_version=os.environ[\"OPENAI_API_VERSION\"],\n",
    "    azure_deployment=os.environ[\"AZURE_GPT4TURBO_DEPLOYMENT\"],\n",
    "    api_key=os.environ[\"AZURE_OPENAI_API_KEY\"]\n",
    ")\n",
    "\n",
    "embed_model = AzureOpenAIEmbedding(\n",
    "    openai_api_version=os.environ[\"OPENAI_API_VERSION\"],\n",
    "    azure_deployment=os.environ[\"AZURE_ADA2_DEPLOYMENT\"],\n",
    "    api_key=os.environ[\"AZURE_OPENAI_API_KEY\"]\n",
    ")\n",
    "\n",
    "# Define service context for GPT-4 for evaluation\n",
    "service_context_gpt4 = ServiceContext.from_defaults(llm=gpt4, embed_model=embed_model)\n",
    "\n",
    "# Define Faithfulness and Relevancy Evaluators which are based on GPT-4\n",
    "faithfulness_gpt4 = FaithfulnessEvaluator(service_context=service_context_gpt4)\n",
    "relevancy_gpt4 = RelevancyEvaluator(service_context=service_context_gpt4)\n",
    "contextrelevancy_gpt4 = ContextRelevancyEvaluator(service_context=service_context_gpt4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def evaluate_response_time_and_accuracy(eval_documents, eval_questions):\n",
    "    \"\"\"\n",
    "    Evaluate the average response time, faithfulness, and relevancy of responses generated by GPT-3.5-turbo for given documents.\n",
    "    \n",
    "    Parameters:\n",
    "    eval_documents (list): The list of pre-split documents to be evaluated.\n",
    "    eval_questions (list): The list of questions to evaluate responses for.\n",
    "    \n",
    "    Returns:\n",
    "    tuple: A tuple containing the average response time, faithfulness, and relevancy metrics.\n",
    "    \"\"\"\n",
    "\n",
    "    total_response_time = 0\n",
    "    total_faithfulness = 0\n",
    "    total_relevancy = 0\n",
    "    total_contextrelevancy = 0.0\n",
    "\n",
    "    # Create vector index\n",
    "    llm = AzureOpenAI(\n",
    "        openai_api_version=os.environ[\"OPENAI_API_VERSION\"],\n",
    "        azure_deployment=os.environ[\"AZURE_GPT3TURBO_DEPLOYMENT\"],\n",
    "        api_key=os.environ[\"AZURE_OPENAI_API_KEY\"]\n",
    "    )\n",
    "    service_context = ServiceContext.from_defaults(llm=llm, embed_model=embed_model)\n",
    "    vector_index = VectorStoreIndex.from_documents(\n",
    "        eval_documents, service_context=service_context\n",
    "    )\n",
    "    # Build query engine\n",
    "    query_engine = vector_index.as_query_engine()\n",
    "    num_questions = len(eval_questions)\n",
    "\n",
    "    # Iterate over each question in eval_questions to compute metrics.\n",
    "    # While BatchEvalRunner can be used for faster evaluations (see: https://docs.llamaindex.ai/en/latest/examples/evaluation/batch_eval.html),\n",
    "    # we're using a loop here to specifically measure response time for different chunk sizes.\n",
    "\n",
    "    df = []\n",
    "\n",
    "    for question in tqdm.tqdm(eval_questions):\n",
    "\n",
    "        start_time = time.time()\n",
    "        response_vector = query_engine.query(question)\n",
    "        elapsed_time = time.time() - start_time\n",
    "        \n",
    "        faithfulness_result = faithfulness_gpt4.evaluate_response(\n",
    "            response=response_vector\n",
    "        ).passing\n",
    "        \n",
    "        relevancy_result = relevancy_gpt4.evaluate_response(\n",
    "            query=question, response=response_vector\n",
    "        ).passing\n",
    "\n",
    "        contextrelevancy_result = await contextrelevancy_gpt4.aevaluate_response(question, response_vector)\n",
    "\n",
    "        total_response_time += elapsed_time\n",
    "        total_faithfulness += faithfulness_result\n",
    "        total_relevancy += relevancy_result\n",
    "        total_contextrelevancy += contextrelevancy_result.score\n",
    "\n",
    "\n",
    "        data = {'question': question, \n",
    "                'response': response_vector.response,\n",
    "                'context': [{'text': c.text, 'metadata': c.metadata} for c in response_vector.source_nodes],\n",
    "                'faithfulness': faithfulness_result, \n",
    "                'relevancy': relevancy_result,\n",
    "                'context_relevancy_score': contextrelevancy_result.score,\n",
    "                'context_relevancy_feedback': contextrelevancy_result.feedback,\n",
    "                'response_time': elapsed_time}\n",
    "\n",
    "        df.append(data.copy())\n",
    "\n",
    "    average_response_time = total_response_time / num_questions\n",
    "    average_faithfulness = total_faithfulness / num_questions\n",
    "    average_relevancy = total_relevancy / num_questions\n",
    "    average_contextrelevancy = total_contextrelevancy / num_questions\n",
    "\n",
    "    return average_response_time, average_faithfulness, average_relevancy, average_contextrelevancy, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "CHUNKERIZATION FILE:  documents_spanisharticlesplitter.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2359/1377853279.py:24: DeprecationWarning: Call to deprecated class method from_defaults. (ServiceContext is deprecated, please use `llama_index.settings.Settings` instead.) -- Deprecated since version 0.10.0.\n",
      "  service_context = ServiceContext.from_defaults(llm=llm, embed_model=embed_model)\n",
      " 68%|██████▊   | 203/300 [33:20<14:14,  8.81s/it] /usr/lib/python3.11/typing.py:395: RuntimeWarning: coroutine 'BaseEvaluator.aevaluate_response' was never awaited\n",
      "  ev_args = tuple(_eval_type(a, globalns, localns, recursive_guard) for a in t.__args__)\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n",
      "100%|██████████| 300/300 [48:30<00:00,  9.70s/it]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv( path + '/eval_questions.csv')\n",
    "eval_questions = df['Questions'].tolist()\n",
    "\n",
    "results = {}\n",
    "directory = path + '/chunks'\n",
    "for filename in os.listdir(directory):\n",
    "    print(\"-----------------------------------\")\n",
    "    print(\"CHUNKERIZATION FILE: \", filename)\n",
    "    file_path = os.path.join(directory, filename)\n",
    "    with open(file_path, \"rb\") as file:\n",
    "        langdocs = pickle.load(file)\n",
    "    # Documents to llama-index docs\n",
    "    llamadocs = [Document(text=doc.page_content, metadata=doc.metadata) for doc in langdocs]\n",
    "    avg_response_time, avg_faithfulness, avg_relevancy, average_contextrelevancy, df = await evaluate_response_time_and_accuracy(llamadocs, eval_questions)\n",
    "    results[filename] = { \"Average response time\": avg_response_time, \"Average faithfulness\": avg_faithfulness, \"Average relevancy\": avg_relevancy , \"Average context relevancy\": average_contextrelevancy}\n",
    "\n",
    "    file_path = os.path.join(path, 'eval_chunks', filename.replace('.pkl', '_answers.json'))\n",
    "    # Save the metrics dictionary as a JSON file\n",
    "    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "        json.dump(df, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "    break\n",
    "\n",
    "\n",
    "file_path = os.path.join(path, 'metrics.json')\n",
    "# Save the metrics dictionary as a JSON file\n",
    "with open(file_path, 'w') as file:\n",
    "    json.dump(results, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
