\cleardoublepage

\chapter{Large Language Models (LLMs)}
\label{Large Language Models (LLMs)}

\section{Introducción}
Los Modelos de Lenguaje de Gran Escala (LLMs) han demostrado capacidades notables en tareas de procesamiento de lenguaje natural y más allá. Estos modelos han revolucionado el campo del NLP y se han postulado como los modelos a resolver el problema de la generación de texto. 
Para este capítulo se ha usado la revisión \textit{A Comprehensive Overview of Large Language Models} \citep{naveed2023comprehensive}

\section{Generación de Texto}
La generación de texto es una de las tareas fundamentales que los LLMs intentan resolver. Este problema consiste en producir secuencias de texto coherente y relevante a partir de una entrada dada. La tarea puede implicar varios subproblemas, como la continuación de un texto, la respuesta a preguntas, la traducción automática, entre otros.

El enfoque principal para resolver la generación de texto en LLMs es la predicción del siguiente token. Esto implica modelar la probabilidad condicional de cada token dado el contexto de los tokens anteriores. Formalmente, si tenemos una secuencia de tokens \( t_1, t_2, ..., t_n \), el modelo aprende a predecir la probabilidad del token \( t_{n+1} \) basándose en los tokens \( t_1, t_2, ..., t_n \). Este proceso se puede expresar como:
\[ P(t_{n+1} | t_1, t_2, ..., t_n) \]
Para entrenar este tipo de modelos, se utilizan corpus grandes de texto en el cual el modelo ajusta sus parámetros para maximizar la probabilidad de los tokens siguientes.

\section{Historia y Evolución de los LLMs}
La evolución de los modelos de lenguaje natural ha sido marcada por avances significativos en las técnicas y arquitecturas utilizadas. Desde los primeros modelos estadísticos hasta los avanzados modelos de lenguaje de gran escala (LLMs), el campo ha experimentado una transformación profunda.

\subsection{Modelos de Lenguaje Estadísticos}
Los primeros enfoques para el procesamiento de lenguaje natural se basaban en modelos estadísticos. Estos modelos utilizaban probabilidades para predecir la aparición de palabras en un contexto dado. Un ejemplo típico es el modelo de n-gramas, que calcula la probabilidad de una palabra basándose en las n-1 palabras anteriores. Aunque efectivos en ciertos contextos, estos modelos tenían limitaciones significativas en su capacidad para capturar dependencias a largo plazo y gestionar vocabularios extensos.

\subsection{Modelos de Lenguaje Neurales}
Con el advenimiento de las redes neuronales, los modelos de lenguaje experimentaron un cambio paradigmático. Los modelos de lenguaje neurales, como los basados en redes neuronales recurrentes (RNNs), ofrecieron mejoras en la capacidad de modelado y generalización. Sin embargo, a pesar de estas mejoras, las RNNs enfrentaban desafíos en la captura de dependencias a largo plazo debido al problema del gradiente desvaneciente. Las arquitecturas como LSTM (Long Short-Term Memory) y GRU (Gated Recurrent Unit) fueron desarrolladas para mitigar estos problemas.

\subsection{Transformers y su Impacto}
El verdadero avance en los modelos de lenguaje vino con la introducción de los Transformers, presentados por Vaswani et al. en 2017 \citep{vaswani2017attention}. Los Transformers se basan en un mecanismo de autoatención que permite al modelo considerar todas las posiciones en la secuencia de entrada simultáneamente, superando las limitaciones de las RNNs en cuanto a la captura de dependencias a largo plazo. Esta arquitectura revolucionó el campo y llevó al desarrollo de modelos preentrenados como BERT (Bidirectional Encoder Representations from Transformers) y GPT (Generative Pre-trained Transformer). La Figura \ref{fig:transformer_architecture} muestra la arquitectura básica de un Transformer.

\begin{figure}[h]
\centering
\includegraphics[width=0.3\textwidth]{figuras/capitulo1/transformer_architecture.png}
\caption{Arquitectura básica de un Transformer.}
\label{fig:transformer_architecture}
\end{figure}

\subsection{Transición de PLMs a LLMs}
La transición de los modelos de lenguaje preentrenados (PLMs) a los modelos grandes del lenguaje (LLMs) se caracterizó por un aumento significativo en el tamaño de los modelos y la cantidad de datos de entrenamiento. Modelos como GPT-3, con 175 mil millones de parámetros, demostraron que el escalado de los modelos y los datos de entrenamiento puede llevar a mejoras drásticas en el rendimiento en una amplia gama de tareas de NLP. Los LLMs han mostrado capacidades sorprendentes en tareas de zero-shot y few-shot learning, donde pueden realizar tareas no vistas durante el entrenamiento con poca o ninguna adaptación adicional. La Figura \ref{fig:llm_evolution} muestra la evolución cronológica de los principales LLMs.

\begin{figure}[h]
\centering
\includegraphics[width=0.85\textwidth]{figuras/capitulo1/llm_evolution.png}
\caption{Evolución cronológica de los principales LLMs.}
\label{fig:llm_evolution}
\end{figure}


\section{Arquitecturas y Modelos Importantes}
En los últimos años, los Modelos grandes del lenguaje (LLMs) han experimentado avances significativos, lo que ha resultado en la creación de arquitecturas y modelos innovadores. Estos modelos no solo han mejorado en términos de capacidad y precisión, sino que también han introducido nuevas formas de abordar problemas complejos en el procesamiento de lenguaje natural. A continuación, se presentan algunas de las arquitecturas históricas y modelos más importantes en el ámbito de los LLMs.

\subsection{T5 (Text-to-Text Transfer Transformer)}
El modelo T5, desarrollado por Google, es un modelo de codificador-decodificador que trata todas las tareas de procesamiento de lenguaje natural (NLP) como problemas de generación de texto. Esto significa que convierte todas las tareas de entrada en una estructura de texto a texto, lo que permite un enfoque unificado para diversas tareas de NLP.

\begin{figure}[h]
\centering
\includegraphics[width=0.85\textwidth]{figuras/capitulo1/t5.png}
\caption{Ejemplo unificado de formación texto a texto, imagen origen de \citep{raffel2020exploring}}
\label{fig:t5}
\end{figure}

\subsection{BERT (Bidirectional Encoder Representations from Transformers)}
BERT, desarrollado por Google \citep{devlin2018bert}, es un modelo basado en transformers que se entrena utilizando un objetivo de modelado de lenguaje enmascarado, donde se predicen tokens enmascarados dentro de una secuencia utilizando el contexto bidireccional de la secuencia completa. Este enfoque permite a BERT capturar relaciones más complejas y contextuales en el texto en comparación con los modelos unidireccionales. La capacidad de BERT para ser finetuneado de manera eficiente sin requerir modificaciones sustanciales en la arquitectura específica de cada tarea ha contribuido significativamente a su adopción y éxito en múltiples dominios de NLP.

\subsection{mT5 (Multilingual T5)}
mT5 es una variante multilingüe del modelo T5, entrenada en el dataset mC4 que abarca 101 idiomas. Este modelo utiliza un vocabulario más amplio de 250,000 tokens para cubrir múltiples lenguajes. Para evitar el sobreajuste o subajuste en un idioma específico, mT5 emplea un procedimiento de muestreo de datos que selecciona muestras de todos los idiomas. Además, durante el afinamiento para tareas específicas utilizando datos en inglés, el modelo puede generar salidas correctas en otros idiomas.

\subsection{GPT-3 (Generative Pre-trained Transformer 3)}
GPT-3, desarrollado por OpenAI, es uno de los modelos de lenguaje más grandes y avanzados hasta la fecha, con 175 mil millones de parámetros. GPT-3 utiliza una arquitectura de transformers similar a GPT-2 pero con atención densa y dispersa en las capas del transformer. Este modelo demostró que el escalado masivo del tamaño del modelo y los datos de entrenamiento puede llevar a mejoras significativas en el rendimiento en una amplia gama de tareas de NLP. GPT-3 es especialmente conocido por su capacidad de realizar tareas en configuraciones de zero-shot, few-shot y one-shot learning, proporcionando respuestas coherentes y contextualmente relevantes. \hfill \break

\begin{figure}[h]
\centering
\includegraphics[width=0.6\textwidth]{figuras/capitulo1/gpt3.png}
\caption{Zero-shot, one-shot y few-shot contrastado con finetunning tradicional. Imagen origen de \citep{brown2020language}}
\label{fig:zero}
\end{figure}


Entre todos estos avances han llegado infinidad de modelos y arquitecturas que han presentado avances significativos en el ámbito del NLP creando un antes y un después en la figura de los LLMs, véase en la figura \ref{fig:llm_evolution}. Desde modelos abiertos como Llama (meta) y Mistral hasta modelos propietarios como GPT4 (OpenAI) o Claude (Anthropic) en sus diferentes versiones.


\section{Aplicaciones de los LLMs}
Los LLMs han demostrado capacidades notables en diversas tareas de procesamiento del lenguaje natural. En esta sección, se describen algunas de las aplicaciones más relevantes de los LLMs.

\subsection{Generación de Texto}
La generación de texto es una de las aplicaciones más prominentes de los LLMs. Estos modelos pueden generar texto coherente y contextualizado en una variedad de estilos y formatos, desde artículos periodísticos hasta poesía y ficción. La capacidad de los LLMs para entender y replicar patrones de lenguaje humano permite su uso en la creación de contenido, asistiendo a escritores y periodistas, y automatizando la redacción de informes y documentos.

\subsection{Resumen Automático}
Los LLMs también se utilizan para la generación de resúmenes automáticos de documentos extensos. Mediante técnicas de comprensión y extracción de información, los modelos pueden condensar textos largos en versiones más breves que retienen los puntos clave. Esto es especialmente útil en campos como el periodismo, la investigación científica y la gestión de información empresarial, donde es necesario procesar grandes cantidades de texto de manera eficiente.

\subsection{Traducción Automática}
Otra aplicación importante de los LLMs es la traducción automática de textos entre diferentes idiomas. Los modelos entrenados en textos multilingües pueden traducir con alta precisión, preservando el contexto y el significado original. Esto facilita la comunicación global y el acceso a información en múltiples idiomas, apoyando tanto a usuarios individuales como a organizaciones en la superación de barreras lingüísticas.

\subsection{Interacción Conversacional}
Los LLMs se utilizan extensamente en la creación de agentes conversacionales y chatbots. Estos agentes pueden participar en diálogos naturales y brindar respuestas relevantes a las consultas de los usuarios. Aplicaciones prácticas incluyen asistentes virtuales, atención al cliente automatizada y soporte técnico. Los avances en los LLMs han permitido que estos sistemas sean más intuitivos y efectivos, mejorando la experiencia del usuario.

\subsection{Respuesta a Preguntas}
Los LLMs son capaces de responder preguntas de manera precisa y contextualizada, utilizando información extraída de grandes bases de datos y textos. Esta capacidad se emplea en sistemas de búsqueda avanzada, asistentes personales y herramientas de aprendizaje, proporcionando respuestas rápidas y precisas a consultas específicas. En el contexto de este trabajo, se desarrollará un agente basado en LLM que pueda responder a preguntas relacionadas con la Constitución Española, demostrando así el potencial de los LLMs en aplicaciones jurídicas y educativas.



\section{Desafíos y Futuras Direcciones}

El desarrollo y la implementación de LLMs presentan una serie de desafíos significativos que deben abordarse para maximizar su eficiencia y eficacia. En esta sección, se analizan algunos de los desafíos más críticos.

\subsection{Costos Computacionales y Energéticos}
Los LLMs requieren enormes recursos computacionales tanto para su entrenamiento como para su implementación. Este alto costo computacional no solo limita su accesibilidad sino que también tiene un impacto ambiental considerable debido al consumo de energía. \citep{strubell2020energy} destacaron que los modelos de aprendizaje profundo en NLP pueden tener un impacto energético significativo, comparable a la huella de carbono de automóviles en su ciclo de vida completo . Por lo tanto, es crucial investigar métodos más eficientes en términos de energía y cómputo, como la cuantización, el pruning y el uso de hardware especializado para reducir estos costos.

\subsection{Privacidad y Datos Sensibles}
El uso de grandes cantidades de datos para entrenar LLMs plantea serias preocupaciones sobre la privacidad. Los modelos pueden inadvertidamente memorizar y reproducir datos sensibles, lo que pone en riesgo la privacidad de los individuos. \citep{brown2022does} discutieron la importancia de definir claramente qué significa preservar la privacidad en el contexto de los modelos de lenguaje y propusieron técnicas para minimizar estos riesgos . La investigación en técnicas de anonimización de datos y el desarrollo de modelos que puedan ser entrenados con datos sintéticos o protegidos son direcciones prometedoras para abordar estas preocupaciones.

\subsection{Alucinaciones}
Los LLMs exhiben \textit{alucinaciones}, donde generan respuestas que, aunque suenen plausibles, son incorrectas o no se alinean con la información proporcionada \citep{zhang2023siren}. Las alucinaciones pueden categorizarse en tres tipos:

\begin{itemize}
    \item \textbf{Alucinación en conflicto con la entrada}: Se produce cuando los LLMs generan contenido que diverge de la información proporcionada por los usuarios.
    \item \textbf{Alucinación en conflicto con el contexto}: Se refiere a cuando los LLMs generan contenido que contradice la información que ellos mismos han generado anteriormente.
    \item \textbf{Alucinación en conflicto con los hechos}: Ocurre cuando los LLMs generan contenido que no se alinea con el conocimiento establecido del mundo.
\end{itemize}

Las alucinaciones representan un desafío significativo para la confiabilidad y precisión de los LLMs. Abordar este problema implica desarrollar técnicas de verificación y validación que permitan a los modelos identificar y corregir estas desviaciones antes de presentar la información al usuario.


