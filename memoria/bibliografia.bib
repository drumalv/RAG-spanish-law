@article{lewis2020retrieval,
  title={Retrieval-augmented generation for knowledge-intensive nlp tasks},
  author={Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and K{\"u}ttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rockt{\"a}schel, Tim and others},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={9459--9474},
  year={2020}
}

@article{langchainretrievers,
  author = {LangChain},
  title = {Documentation for Retriever Modules https://python.langchain.com},
  year = {2024},
  url = {https://python.langchain.com/docs/modules/data_connection/retrievers/},
  urldate = {2024-04-30}
}

@misc{liu2023lost,
      title={Lost in the Middle: How Language Models Use Long Contexts}, 
      author={Nelson F. Liu and Kevin Lin and John Hewitt and Ashwin Paranjape and Michele Bevilacqua and Fabio Petroni and Percy Liang},
      year={2023},
      eprint={2307.03172},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{raffel2020exploring,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={Journal of machine learning research},
  volume={21},
  number={140},
  pages={1--67},
  year={2020}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@article{naveed2023comprehensive,
  title={A comprehensive overview of large language models},
  author={Naveed, Humza and Khan, Asad Ullah and Qiu, Shi and Saqib, Muhammad and Anwar, Saeed and Usman, Muhammad and Barnes, Nick and Mian, Ajmal},
  journal={arXiv preprint arXiv:2307.06435},
  year={2023}
}

@inproceedings{strubell2020energy,
  title={Energy and policy considerations for modern deep learning research},
  author={Strubell, Emma and Ganesh, Ananya and McCallum, Andrew},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={34},
  number={09},
  pages={13693--13696},
  year={2020}
}

@inproceedings{brown2022does,
  title={What does it mean for a language model to preserve privacy?},
  author={Brown, Hannah and Lee, Katherine and Mireshghallah, Fatemehsadat and Shokri, Reza and Tram{\`e}r, Florian},
  booktitle={Proceedings of the 2022 ACM conference on fairness, accountability, and transparency},
  pages={2280--2292},
  year={2022}
}

@article{zhang2023siren,
  title={Siren's song in the AI ocean: a survey on hallucination in large language models},
  author={Zhang, Yue and Li, Yafu and Cui, Leyang and Cai, Deng and Liu, Lemao and Fu, Tingchen and Huang, Xinting and Zhao, Enbo and Zhang, Yu and Chen, Yulong and others},
  journal={arXiv preprint arXiv:2309.01219},
  year={2023}
}

@article{gao2023retrieval,
  title={Retrieval-augmented generation for large language models: A survey},
  author={Gao, Yunfan and Xiong, Yun and Gao, Xinyu and Jia, Kangxiang and Pan, Jinliu and Bi, Yuxi and Dai, Yi and Sun, Jiawei and Wang, Haofen},
  journal={arXiv preprint arXiv:2312.10997},
  year={2023}
}

@article{cloudgirl2023rag,
    author = {Vergadia, Priyanka},
    title = {The Secret Sauce of RAG: Vector Search and Embeddings, https://www.thecloudgirl.dev/},
    year = {2023},
    url = {https://www.thecloudgirl.dev/}
}

@article{datastax2023rag,
	author = {datastax},
    title = {Retrieval-augmented Generation (RAG): A Comprehensive Guide, https://www.datastax.com/},
    year = {2023},
    url = {https://www.datastax.com/}
}

@article{nvidia2023rag,
	author = {Nvidia},
    title = {RAG 101: Retrieval-Augmented Generation Questions Answered, https://developer.nvidia.com/},
    year = {2023},
    url = {https://developer.nvidia.com/}
}

@article{nanonets2023rag,
	author = {Nanonets},
    title = {Building a Retrieval-Augmented Generation (RAG) App, https://nanonets.com/},
    year = {2023},
    url = {https://nanonets.com/}
}

@article{artificialanalysis,
	author = {Artificialanalysis},
    title = {Independent analysis of AI models and API providers, https://artificialanalysis.ai/},
    year = {2024},
    url = {https://artificialanalysis.ai/}
}

@article{openai,
	author = {OpenAI},
    title = {OpenAI documentation, https://openai.com/},
    year = {2024},
    url = {https://openai.com/}
}

@online{leadhugging,
    title = {Leaderboard MTEB huggingface, https://huggingface.co/spaces/mteb/leaderboard},
    year = {2024},
    url = {https://huggingface.co/spaces/mteb/leaderboard}
}

@article{llamaindex-blog,
	author = {LlamaIndex},
    title = {Evaluating the Ideal Chunk Size for a RAG System using LlamaIndex, https://www.llamaindex.ai/blog/evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5},
    year = {2023},
    url = {https://www.llamaindex.ai/blog/evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5}
}

@article{datacamp-eval,
	author = {DataCamp},
    title = {How to Improve RAG Performance: 5 Key Techniques with Examples, https://www.datacamp.com/community/tutorials/improve-rag-performance-techniques},
    year = {2023},
    url = {https://www.datacamp.com/community/tutorials/improve-rag-performance-techniques}
}

@article{vectorize-eval,
	author = {Vectorize.io},
    title = {Evaluating the ideal chunk size for a rag system, https://www.vectorize.io/blog/evaluating-the-ideal-chunk-size-for-a-rag-system},
    year = {2023},
    url = {https://www.vectorize.io/blog/evaluating-the-ideal-chunk-size-for-a-rag-system}
}

@article{ragas2023,
    title = {RAGAS: Automated Evaluation of Retrieval Augmented Generation},
    author = {Shahul Es and Jithin James and Luis Espinosa-Anke and Steven Schockaert},
    journal = {arXiv preprint arXiv:2309.15217},
    year = {2023},
    url = {https://arxiv.org/abs/2309.15217}
}



